"use strict";(self.webpackChunkharvester_docs=self.webpackChunkharvester_docs||[]).push([[60316],{56329:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>l});a(67294);var n=a(3905);const r={sidebar_position:4,sidebar_label:"Upgrade from v1.1.2 to v1.2.0 (not recommended)",title:"Upgrade from v1.1.2 to v1.2.0 (not recommended)"},i=void 0,o={unversionedId:"upgrade/v1-1-2-to-v1-2-0",id:"version-v1.2/upgrade/v1-1-2-to-v1-2-0",title:"Upgrade from v1.1.2 to v1.2.0 (not recommended)",description:"Due to the known issues found v1.2.0:",source:"@site/versioned_docs/version-v1.2/upgrade/v1-1-2-to-v1-2-0.md",sourceDirName:"upgrade",slug:"/upgrade/v1-1-2-to-v1-2-0",permalink:"/v1.2/upgrade/v1-1-2-to-v1-2-0",draft:!1,editUrl:"https://github.com/harvester/docs/edit/main/versioned_docs/version-v1.2/upgrade/v1-1-2-to-v1-2-0.md",tags:[],version:"v1.2",lastUpdatedAt:1721701671,formattedLastUpdatedAt:"Jul 23, 2024",sidebarPosition:4,frontMatter:{sidebar_position:4,sidebar_label:"Upgrade from v1.1.2 to v1.2.0 (not recommended)",title:"Upgrade from v1.1.2 to v1.2.0 (not recommended)"},sidebar:"api",previous:{title:"Upgrade from v1.1.2/v1.1.3/v1.2.0 to v1.2.1",permalink:"/v1.2/upgrade/v1-2-0-to-v1-2-1"},next:{title:"Upgrade from v1.1.1/v1.1.2 to v1.1.3",permalink:"/v1.2/upgrade/v1-1-1-to-v1-1-3"}},s={},l=[{value:"General information",id:"general-information",level:2},{value:"Known issues",id:"known-issues",level:2},{value:"1. An upgrade can&#39;t start and reports <code>&quot;validator.harvesterhci.io&quot; denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready</code>",id:"1-an-upgrade-cant-start-and-reports-validatorharvesterhciio-denied-the-request-managed-chart-rancher-monitoring-is-not-ready-please-wait-for-it-to-be-ready",level:3},{value:"2. An upgrade is stuck in <code>Creating Upgrade Repository</code>",id:"2-an-upgrade-is-stuck-in-creating-upgrade-repository",level:3},{value:"3. An upgrade is stuck when pre-draining a node",id:"3-an-upgrade-is-stuck-when-pre-draining-a-node",level:3},{value:"4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline",id:"4-an-upgrade-is-stuck-in-upgrading-the-first-node-job-was-active-longer-than-the-specified-deadline",level:3},{value:"5. An upgrade is stuck in the Pre-drained state",id:"5-an-upgrade-is-stuck-in-the-pre-drained-state",level:3},{value:"5.1 The node contains a Longhorn <code>instance-manager-r</code> pod that serves single-replica volume(s)",id:"51-the-node-contains-a-longhorn-instance-manager-r-pod-that-serves-single-replica-volumes",level:4},{value:"5.2 Misconfigured Longhorn <code>instance-manager-r</code> Pod Disruption Budgets (PDB)",id:"52-misconfigured-longhorn-instance-manager-r-pod-disruption-budgets-pdb",level:4},{value:"5.3 The <code>instance-manager-e</code> pod could not be drained",id:"53-the-instance-manager-e-pod-could-not-be-drained",level:4},{value:"6. An upgrade is stuck in the Upgrading System Service state",id:"6-an-upgrade-is-stuck-in-the-upgrading-system-service-state",level:3},{value:"POD prometheus-rancher-monitoring-prometheus-0 is to be deleted",id:"pod-prometheus-rancher-monitoring-prometheus-0-is-to-be-deleted",level:4},{value:"Multiple PODs in cattle-monitoring-system namespace are to be deleted",id:"multiple-pods-in-cattle-monitoring-system-namespace-are-to-be-deleted",level:4},{value:"7. Upgrade stuck in the <code>Upgrading System Service</code> state",id:"7-upgrade-stuck-in-the-upgrading-system-service-state",level:3},{value:"8. The <code>registry.suse.com/harvester-beta/vmdp:latest</code> image is not available in air-gapped environment",id:"8-the-registrysusecomharvester-betavmdplatest-image-is-not-available-in-air-gapped-environment",level:3},{value:"9. An Upgrade is stuck in the Post-draining state",id:"9-an-upgrade-is-stuck-in-the-post-draining-state",level:3},{value:"10. An upgrade is stuck in the Upgrading System Service state due to the <code>customer provided SSL certificate without IP SAN</code> error in <code>fleet-agent</code>",id:"10-an-upgrade-is-stuck-in-the-upgrading-system-service-state-due-to-the-customer-provided-ssl-certificate-without-ip-san-error-in-fleet-agent",level:3},{value:"11. An upgrade is denied due to <code>managed chart rancher-monitoring-crd is not ready</code>",id:"11-an-upgrade-is-denied-due-to-managed-chart-rancher-monitoring-crd-is-not-ready",level:3}],p={toc:l},d="wrapper";function m({components:e,...t}){return(0,n.kt)(d,{...p,...t,components:e,mdxType:"MDXLayout"},(0,n.kt)("head",null,(0,n.kt)("link",{rel:"canonical",href:"https://docs.harvesterhci.io/v1.2/upgrade/v1-1-2-to-v1-2-0"})),(0,n.kt)("admonition",{type:"caution"},(0,n.kt)("p",{parentName:"admonition"},"Due to the known issues found v1.2.0:"),(0,n.kt)("ul",{parentName:"admonition"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"#9-an-upgrade-is-stuck-in-the-post-draining-state"},"An Upgrade is stuck in the Post-draining state")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"#10-an-upgrade-is-stuck-in-the-upgrading-system-service-state-due-to-the-customer-provided-ssl-certificate-without-ip-san-error-in-fleet-agent"},"An upgrade is stuck in the Upgrading System Service state due to the customer provided SSL certificate without IP SAN error in fleet-agent"))),(0,n.kt)("p",{parentName:"admonition"},"We don't recommend upgrading to v1.2.0. Please upgrade your v1.1.x cluster to v1.2.1.")),(0,n.kt)("h2",{id:"general-information"},"General information"),(0,n.kt)("admonition",{type:"tip"},(0,n.kt)("p",{parentName:"admonition"},"Before you start an upgrade, you can run the pre-check script to make sure the cluster is in a stable state. For more details, please visit this ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/harvester/upgrade-helpers/tree/main/pre-check/v1.1.x"},"URL")," for the script.")),(0,n.kt)("p",null,"Once there is an upgradable version, the Harvester GUI Dashboard page will show an upgrade button. For more details, please refer to ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/upgrade/index#start-an-upgrade"},"start an upgrade"),"."),(0,n.kt)("p",null,"For the air-gap env upgrade, please refer to ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/upgrade/index#prepare-an-air-gapped-upgrade"},"prepare an air-gapped upgrade"),"."),(0,n.kt)("h2",{id:"known-issues"},"Known issues"),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"1-an-upgrade-cant-start-and-reports-validatorharvesterhciio-denied-the-request-managed-chart-rancher-monitoring-is-not-ready-please-wait-for-it-to-be-ready"},"1. An upgrade can't start and reports ",(0,n.kt)("inlineCode",{parentName:"h3"},'"validator.harvesterhci.io" denied the request: managed chart rancher-monitoring is not ready, please wait for it to be ready')),(0,n.kt)("p",null,"If a cluster is configured with a ",(0,n.kt)("strong",{parentName:"p"},"storage network"),", an upgrade can't start with the following message."),(0,n.kt)("p",null,(0,n.kt)("img",{src:a(25067).Z,width:"1722",height:"606"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/3839"},"[Doc] upgrade stuck while upgrading system service with alertmanager and prometheus")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192"},"https://github.com/harvester/harvester/issues/3839#issuecomment-1534438192"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"2-an-upgrade-is-stuck-in-creating-upgrade-repository"},"2. An upgrade is stuck in ",(0,n.kt)("inlineCode",{parentName:"h3"},"Creating Upgrade Repository")),(0,n.kt)("p",null,"During an upgrade, ",(0,n.kt)("strong",{parentName:"p"},"Creating Upgrade Repository")," is stuck in the ",(0,n.kt)("strong",{parentName:"p"},"Pending")," state:"),(0,n.kt)("p",null,"  ",(0,n.kt)("img",{src:a(65894).Z,width:"1142",height:"286"})),(0,n.kt)("p",null,"Please perform the following steps to check if the cluster runs into the issue:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the upgrade repository pod:"),(0,n.kt)("p",{parentName:"li"},(0,n.kt)("img",{src:a(83918).Z,width:"1375",height:"185"})),(0,n.kt)("p",{parentName:"li"},"If the ",(0,n.kt)("inlineCode",{parentName:"p"},"virt-launcher-upgrade-repo-hvst-<upgrade-name>")," pod stays in ",(0,n.kt)("inlineCode",{parentName:"p"},"ContainerCreating"),", your cluster might have run into this issue. In this case, proceed with step 2.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the upgrade repository volume in the Longhorn GUI."),(0,n.kt)("ol",{parentName:"li"},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},(0,n.kt)("a",{parentName:"p",href:"/v1.2/troubleshooting/harvester#access-embedded-rancher-and-longhorn-dashboards"},"Go to Longhorn GUI"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Navigate to the ",(0,n.kt)("strong",{parentName:"p"},"Volume")," page.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the upgrade repository VM volume. It should be attached to a pod called ",(0,n.kt)("inlineCode",{parentName:"p"},"virt-launcher-upgrade-repo-hvst-<upgrade-name>"),". If one of the volume's replicas stays in ",(0,n.kt)("inlineCode",{parentName:"p"},"Stopped")," (gray color), the cluster is running into the issue."),(0,n.kt)("p",{parentName:"li"},(0,n.kt)("img",{src:a(84940).Z,width:"1883",height:"588"})))))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4246"},"[BUG] upgrade stuck on create upgrade VM")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Delete the ",(0,n.kt)("inlineCode",{parentName:"li"},"Stopped")," replica from Longhorn GUI. Or,"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"/v1.2/upgrade/troubleshooting#start-over-an-upgrade"},"Start over the upgrade"),".")))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"3-an-upgrade-is-stuck-when-pre-draining-a-node"},"3. An upgrade is stuck when pre-draining a node"),(0,n.kt)("p",null,'Starting from v1.1.0, Harvester will wait for all volumes to become healthy (when node count >= 3) before upgrading a node. Generally, you can check volumes\' health if an upgrade is stuck in the "pre-draining" state.'),(0,n.kt)("p",null,"Visit ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/troubleshooting/harvester#access-embedded-rancher-and-longhorn-dashboards"},'"Access Embedded Longhorn"')," to see how to access the embedded Longhorn GUI."),(0,n.kt)("p",null,"You can also check the pre-drain job logs. Please refer to ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/upgrade/troubleshooting#phase-4-upgrade-nodes"},"Phase 4: Upgrade nodes")," in the troubleshooting guide."),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"4-an-upgrade-is-stuck-in-upgrading-the-first-node-job-was-active-longer-than-the-specified-deadline"},"4. An upgrade is stuck in upgrading the first node: Job was active longer than the specified deadline"),(0,n.kt)("p",null,"An upgrade fails, as shown in the screenshot below:"),(0,n.kt)("p",null,(0,n.kt)("img",{src:a(4947).Z,width:"1140",height:"918"})),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/2894"},"[BUG] Upgrade stuck in upgrading first node: Job was active longer than specified deadline")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690"},"https://github.com/harvester/harvester/issues/2894#issuecomment-1274069690"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"5-an-upgrade-is-stuck-in-the-pre-drained-state"},"5. An upgrade is stuck in the Pre-drained state"),(0,n.kt)("p",null,'You might see an upgrade is stuck in the "pre-drained" state:'),(0,n.kt)("p",null,(0,n.kt)("img",{src:a(30150).Z,width:"1130",height:"530"})),(0,n.kt)("p",null,"In this stage, Kubernetes is supposed to drain the workload on the node, but some reasons might cause the process to stall."),(0,n.kt)("h4",{id:"51-the-node-contains-a-longhorn-instance-manager-r-pod-that-serves-single-replica-volumes"},"5.1 The node contains a Longhorn ",(0,n.kt)("inlineCode",{parentName:"h4"},"instance-manager-r")," pod that serves single-replica volume(s)"),(0,n.kt)("p",null,"Longhorn doesn't allow draining a node if the node contains the last surviving replica of a volume. To check if a node is running into this situation, follow these steps: "),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"List single-replica volumes with the command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + \"/\" + .metadata.name'\n")),(0,n.kt)("p",{parentName:"li"},"For example:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl get volumes.longhorn.io -A -o yaml | yq '.items[] | select(.spec.numberOfReplicas == 1) | .metadata.namespace + \"/\" + .metadata.name'\nlonghorn-system/pvc-d1f19bab-200e-483b-b348-c87cfbba85ab\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check if the replica resides on the stuck node:"),(0,n.kt)("p",{parentName:"li"},"List the NodeID of the volume's replica with the command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == \"<volume>\") | .spec.nodeID'\n")),(0,n.kt)("p",{parentName:"li"},"For example:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl get replicas.longhorn.io -n longhorn-system -o yaml | yq '.items[] | select(.metadata.labels.longhornvolume == \"pvc-d1f19bab-200e-483b-b348-c87cfbba85ab\") | .spec.nodeID'\nnode1\n")),(0,n.kt)("p",{parentName:"li"},"If the result shows that the replica resides on the node where the upgrade is stuck (in this example, node1), your cluster is hitting this issue."))),(0,n.kt)("p",null,"There are a couple of ways to address this situation. Choose the most appropriate method for your VM:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Shut down the VM that uses the single-replica volume to detach the volume, allowing the upgrade to continue."),(0,n.kt)("li",{parentName:"ol"},"Adjust the volumes's replicas to more than one.",(0,n.kt)("ol",{parentName:"li"},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("a",{parentName:"li",href:"/v1.2/troubleshooting/harvester#access-embedded-rancher-and-longhorn-dashboards"},"Go to Longhorn GUI"),"."),(0,n.kt)("li",{parentName:"ol"},"Go to the ",(0,n.kt)("strong",{parentName:"li"},"Volume")," page."),(0,n.kt)("li",{parentName:"ol"},"Locate the problematic volume and click the icon on the right side, then select ",(0,n.kt)("strong",{parentName:"li"},"Update Replicas Count"),":\n",(0,n.kt)("img",{src:a(35710).Z,width:"3708",height:"594"})),(0,n.kt)("li",{parentName:"ol"},"Increase the ",(0,n.kt)("strong",{parentName:"li"},"Number of Replicas")," and select ",(0,n.kt)("strong",{parentName:"li"},"OK"),".")))),(0,n.kt)("h4",{id:"52-misconfigured-longhorn-instance-manager-r-pod-disruption-budgets-pdb"},"5.2 Misconfigured Longhorn ",(0,n.kt)("inlineCode",{parentName:"h4"},"instance-manager-r")," Pod Disruption Budgets (PDB)"),(0,n.kt)("p",null,"A misconfigured PDB could cause this issue. To check if that's the case, perform the following steps:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Assume the stuck node is ",(0,n.kt)("inlineCode",{parentName:"p"},"harvester-node-1"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e")," or ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-r")," pod names on the stuck node:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl get pods -n longhorn-system --field-selector spec.nodeName=harvester-node-1 | grep instance-manager\ninstance-manager-r-d4ed2788          1/1     Running   0              3d8h\n")),(0,n.kt)("p",{parentName:"li"},"The output above shows that the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-r-d4ed2788")," pod is on the node. ")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check Rancher logs and verify that the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e")," or ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-r")," pod can't be drained:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'$ kubectl logs deployment/rancher -n cattle-system\n...\n2023-03-28T17:10:52.199575910Z 2023/03/28 17:10:52 [INFO] [planner] rkecluster fleet-local/local: waiting: draining etcd node(s) custom-4f8cb698b24a,custom-a0f714579def\n2023-03-28T17:10:55.034453029Z evicting pod longhorn-system/instance-manager-r-d4ed2788\n2023-03-28T17:10:55.080933607Z error when evicting pods/"instance-manager-r-d4ed2788" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod\'s disruption budget.\n'))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Run the command to check if there is a PDB associated with the stuck node:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'$ kubectl get pdb -n longhorn-system -o yaml | yq \'.items[] | select(.spec.selector.matchLabels."longhorn.io/node"=="harvester-node-1") | .metadata.name\'\ninstance-manager-r-466e3c7f\n'))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the owner of the instance manager to this PDB:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl get instancemanager instance-manager-r-466e3c7f -n longhorn-system -o yaml | yq -e '.spec.nodeID'\nharvester-node-2\n")),(0,n.kt)("p",{parentName:"li"},"If the output doesn't match the stuck node (in this example output, ",(0,n.kt)("inlineCode",{parentName:"p"},"harvester-node-2")," doesn't match the stuck node ",(0,n.kt)("inlineCode",{parentName:"p"},"harvester-node-1"),"), then we can conclude this issue happens.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Before applying the workaround, check if all volumes are healthy:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get volumes -n longhorn-system -o yaml | yq '.items[] | select(.status.state == \"attached\")| .status.robustness'\n")),(0,n.kt)("p",{parentName:"li"},"The output should all be ",(0,n.kt)("inlineCode",{parentName:"p"},"healthy"),". If this is not the case, you might want to uncordon nodes to make the volume healthy again.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Remove the misconfigured PDB:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl delete pdb instance-manager-r-466e3c7f -n longhorn-system\n")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/3730"},"[BUG] 3 Node AirGapped Cluster Upgrade Stuck v1.1.0->v1.1.2-rc4"))))),(0,n.kt)("h4",{id:"53-the-instance-manager-e-pod-could-not-be-drained"},"5.3 The ",(0,n.kt)("inlineCode",{parentName:"h4"},"instance-manager-e")," pod could not be drained"),(0,n.kt)("p",null,"During an upgrade, you might encounter an issue where you can't drain the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e")," pod. When this situation occurs, you will see error messages in the Rancher logs like the ones shown below:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'$ kubectl logs deployment/rancher -n cattle-system | grep "evicting pod"\nevicting pod longhorn-system/instance-manager-r-a06a43f3437ab4f643eea7053b915a80\nevicting pod longhorn-system/instance-manager-e-452e87d2\nerror when evicting pods/"instance-manager-r-a06a43f3437ab4f643eea7053b915a80" -n "Longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod\'s disruption budget.\nerror when evicting pods/"instance-manager-e-452e87d2" -n "longhorn-system" (will retry after 5s): Cannot evict pod as it would violate the pod\'s disruption budget.\n')),(0,n.kt)("p",null,"Check the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e")," to see if any engine instances remain."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'$ kubectl get instancemanager instance-manager-e-452e87d2 -n longhorn-system -o yaml | yq -e ".status.instances"\npvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57:\n  spec:\n    name: pvc-7b120d60-1577-4716-be5a-62348271025a-e-1cd53c57\n  status:\n    endpoint: ""\n    errorMsg: ""\n    listen: ""\n    portEnd: 10001\n    portStart: 10001\n    resourceVersion: 0\n    state: running\n    type: ""\n')),(0,n.kt)("p",null,"In this example, the ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e-452e87d2")," still has an engine instance, so you can't drain the pod."),(0,n.kt)("p",null,"You need to check the engine numbers to see if any engine number is redundant. Each PVC should only have one engine."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"# kubectl get engines -n longhorn-system -l longhornvolume=pvc-7b120d60-1577-4716-be5a-62348271025a\nNAME                                                  STATE     NODE               INSTANCEMANAGER                                      IMAGE                               AGE\npvc-76120d60-1577-4716-be5a-62348271025a-e-08220662   running   harvester-qv4hd    instance-manager-e-625d715e2f2e7065d64339f9b31407c2  longhornio/longhorn-engine:v1.4.3   2d12h\npvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57   running   harvester-lhlkv    instance-manager-e-452e87d2                          longhornio/longhorn-engine:v1.4.3   4d10h\n")),(0,n.kt)("p",null,"The example above shows that two engines exist for the same PVC, which is a known issue in Longhorn ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/longhorn/longhorn/issues/6642"},"#6642"),". To resolve this, delete the redundant engine to allow the upgrade to continue. "),(0,n.kt)("p",null,"To determine which engine is the correct one, use the following command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ kubectl get volumes pvc-7b120d60-1577-4716-be5a-62348271025a -n longhorn-system\nNAME                                      STATE     ROBUSTNESS  SCHEDULED SIZE        NODE            AGE\npvc-7b120d60-1577-4716-be5a-62348271025a  attached  healthy               42949672960 harvester-q4vhd 4d10h\n")),(0,n.kt)("p",null,"In this example, the volume ",(0,n.kt)("inlineCode",{parentName:"p"},"pvc-7b120d60-1577-4716-be5a-62348271025a")," is active on the node ",(0,n.kt)("inlineCode",{parentName:"p"},"harvester-q4vhd"),", indicating that the engine not running on this node is redundant."),(0,n.kt)("p",null,"To make the engine inactive and trigger its automatic deletion by Longhorn, run the following command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'$ kubectl patch engine pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 -n longhorn-system --type=\'json\' -p=\'[{"op": "replace", "path": "/spec/active", "value": false}]\'\nengine.longhorn.io/pvc-7b120d60-1577-4716-be5a-62348271025a-e-lcd53c57 patched\n')),(0,n.kt)("p",null,"After a few seconds, you can verify the engine's status:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"$ kubectl get engine -n longhorn-system|grep pvc-7b120d60-1577-4716-be5a-62348271025a\npvc-7b120d60-1577-4716-be5a-62348271025a-e-08220b62   running  harvester-q4vhd   instance-manager-e-625d715e2f2e7065d64339f9631407c2  longhornio/longhorn-engine:v1.4.3   2d13h\n")),(0,n.kt)("p",null,"The ",(0,n.kt)("inlineCode",{parentName:"p"},"instance-manager-e")," pod should now drain successfully, allowing the upgrade to proceed."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4477"},"[BUG] Upgrade (v1.1.2 -> v1.2.0-rc6) stuck in pre-drained"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"6-an-upgrade-is-stuck-in-the-upgrading-system-service-state"},"6. An upgrade is stuck in the Upgrading System Service state"),(0,n.kt)("p",null,"If you notice the upgrade is stuck in the ",(0,n.kt)("strong",{parentName:"p"},"Upgrading System Service")," state for a long period of time, you might need to investigate if the upgrade is stuck in the ",(0,n.kt)("inlineCode",{parentName:"p"},"apply-manifests")," phase."),(0,n.kt)("p",null,(0,n.kt)("img",{src:a(42702).Z,width:"1134",height:"1152"})),(0,n.kt)("h4",{id:"pod-prometheus-rancher-monitoring-prometheus-0-is-to-be-deleted"},"POD prometheus-rancher-monitoring-prometheus-0 is to be deleted"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the log of the ",(0,n.kt)("inlineCode",{parentName:"p"},"apply-manifests")," pod to see if the following messages repeat."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl -n harvester-system logs hvst-upgrade-md6wr-apply-manifests-wqslg --tail=10\nTue Sep  5 10:20:39 UTC 2023\nthere are still 1 pods in cattle-monitoring-system to be deleted\nTue Sep  5 10:20:45 UTC 2023\nthere are still 1 pods in cattle-monitoring-system to be deleted\nTue Sep  5 10:20:50 UTC 2023\nthere are still 1 pods in cattle-monitoring-system to be deleted\nTue Sep  5 10:20:55 UTC 2023\nthere are still 1 pods in cattle-monitoring-system to be deleted\nTue Sep  5 10:21:00 UTC 2023\nthere are still 1 pods in cattle-monitoring-system to be deleted\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check if the ",(0,n.kt)("inlineCode",{parentName:"p"},"prometheus-rancher-monitoring-prometheus-0")," pod is stuck with the status ",(0,n.kt)("inlineCode",{parentName:"p"},"Terminating"),"."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl -n cattle-monitoring-system get pods\nNAME                                         READY   STATUS        RESTARTS   AGE\nprometheus-rancher-monitoring-prometheus-0   0/3     Terminating   0          19d\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Find the UID of the terminating pod with the following command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"$ kubectl -n cattle-monitoring-system get pod prometheus-rancher-monitoring-prometheus-0 -o jsonpath='{.metadata.uid}'\n33f43165-6faa-4648-927d-69097901471c\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Get access to any node of the cluster via the console or SSH.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Search for the related log messages in ",(0,n.kt)("inlineCode",{parentName:"p"},"/var/lib/rancher/rke2/agent/logs/kubelet.log")," using the pod's UID."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'E0905 10:26:18.769199   17399 reconciler.go:208] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \\"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\" (UniqueName: \\"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\") pod \\"33f43165-6faa-4648-927d-69097901471c\\" (UID: \\"33f43165-6faa-4648-927d-69097901471c\\") : UnmountVolume.NewUnmounter failed for volume \\"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\" (UniqueName: \\"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\") pod \\"33f43165-6faa-4648-927d-69097901471c\\" (UID: \\"33f43165-6faa-4648-927d-69097901471c\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \\"pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\" (UniqueName: \\"kubernetes.io/csi/driver.longhorn.io^pvc-7781c988-c35b-4cf8-89e6-f2907ef33603\\") pod \\"33f43165-6faa-4648-927d-69097901471c\\" (UID: \\"33f43165-6faa-4648-927d-69097901471c\\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json]: open /var/lib/kubelet/pods/33f43165-6faa-4648-927d-69097901471c/volumes/kubernetes.io~csi/pvc-7781c988-c35b-4cf8-89e6-f2907ef33603/vol_data.json: no such file or directory"\n')),(0,n.kt)("p",{parentName:"li"}," If kubelet continues to complain about the volume failing to unmount, apply the following workaround to allow the upgrade to proceed.")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Forcibly remove the pod stuck with the status ",(0,n.kt)("inlineCode",{parentName:"p"},"Terminating")," with the following command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl delete pod prometheus-rancher-monitoring-prometheus-0 -n cattle-monitoring-system  --force\n")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4484"},"[BUG] The rancher-monitoring Pod stuck at terminating status when upgrading from v1.1.2 to v1.2.0-rc6"))))),(0,n.kt)("h4",{id:"multiple-pods-in-cattle-monitoring-system-namespace-are-to-be-deleted"},"Multiple PODs in cattle-monitoring-system namespace are to be deleted"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the log of the ",(0,n.kt)("inlineCode",{parentName:"p"},"apply-manifests")," pod to see if the following messages repeat."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"there are still 10 pods in cattle-monitoring-system to be deleted\nFri Dec  8 19:06:56 UTC 2023\nthere are still 10 pods in cattle-monitoring-system to be deleted\nFri Dec  8 19:07:01 UTC 2023\n")),(0,n.kt)("p",{parentName:"li"},"When it continues to show 10 (or other number) pods, it encounters below issue."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"The monitoring feature is deployed from the rancher-monitoring ManagedChart, in Harvester v1.2.0,v1.2.1,\nthis ManagedChart is converted to Harvester Addon feature when upgrading.\nThe ManagedChart rancher-monitoring is deleted, normally, all the generated resources including deployment, \ndaemonset etc. will be deleted automatically. But in this case, those resources are not deleted.\nThe above log reflects the result.\nFollowing instructions will guide to delete them manually.\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Locate the affected resources in the ",(0,n.kt)("inlineCode",{parentName:"p"},"cattle-monitoring-system")," namespace."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"Root level resources in cattle-monitoring-system\n\nCustomized CRD: Prometheus\n  Object: rancher-monitoring-prometheus\n  Sub-object: statefulset.apps/prometheus-rancher-monitoring-prometheus\n\nCustomized CRD: Alertmanager\n  object:  rancher-monitoring-alertmanager \n  Sub-object:  statefulset.apps/alertmanager-rancher-monitoring-alertmanager\n\nDeployment:\n  rancher-monitoring-grafana\n  rancher-monitoring-kube-state-metrics\n  rancher-monitoring-operator\n  rancher-monitoring-prometheus-adapter\n\nDaemonset:\n  rancher-monitoring-prometheus-node-exporter\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Delete the affected resources."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"Use below commands to delete them, meanwhile check the log of the `apply-manifests` until it does not\nreport `there are still x pods in cattle-monitoring-system to be deleted`.\n\nkubectl delete prometheus rancher-monitoring-prometheus -n cattle-monitoring-system\nkubectl delete alertmanager rancher-monitoring-alertmanager -n cattle-monitoring-system\n\nkubectl delete deployment rancher-monitoring-grafana -n cattle-monitoring-system\nkubectl delete deployment rancher-monitoring-kube-state-metrics -n cattle-monitoring-system\nkubectl delete deployment rancher-monitoring-operator -n cattle-monitoring-system\nkubectl delete deployment rancher-monitoring-prometheus-adapter -n cattle-monitoring-system\n\nkubectl delete daemonset rancher-monitoring-prometheus-node-exporter -n cattle-monitoring-system\n")),(0,n.kt)("admonition",{parentName:"li",type:"note"},(0,n.kt)("p",{parentName:"admonition"},"You may need to run some of the commands more than once to completely delete the resources.")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4846"},"[BUG] upgrade hung on apply-manifests"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"7-upgrade-stuck-in-the-upgrading-system-service-state"},"7. Upgrade stuck in the ",(0,n.kt)("inlineCode",{parentName:"h3"},"Upgrading System Service")," state"),(0,n.kt)("p",null,"If an upgrade is stuck in an ",(0,n.kt)("inlineCode",{parentName:"p"},"Upgrading System Service")," state for an extended period, some system services' certificates may have expired. To investigate and resolve this issue, follow these steps:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Find the ",(0,n.kt)("inlineCode",{parentName:"p"},"apply-manifest")," job's name with the command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get jobs -n harvester-system -l harvesterhci.io/upgradeComponent=manifest\n")),(0,n.kt)("p",{parentName:"li"},"Example output:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"NAME                                 COMPLETIONS   DURATION   AGE\nhvst-upgrade-9gmg2-apply-manifests   0/1           46s        46s\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the job's log with the command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl logs jobs/hvst-upgrade-9gmg2-apply-manifests -n harvester-system\n")),(0,n.kt)("p",{parentName:"li"},"If the following messages appear in the log, continue to the next step:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"Waiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...\nWaiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...\nWaiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...\nWaiting for CAPI cluster fleet-local/local to be provisioned (current phase: Provisioning, current generation: 30259)...\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check CAPI cluster's state with the command:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get clusters.provisioning.cattle.io local -n fleet-local -o yaml\n")),(0,n.kt)("p",{parentName:"li"},"If you see a condition similar to the one below, it's likely that the cluster has encountered the issue:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"    - lastUpdateTime: \"2023-01-17T16:26:48Z\"\n      message: 'configuring bootstrap node(s) custom-24cb32ce8387: waiting for probes:\n        kube-controller-manager, kube-scheduler'\n      reason: Waiting\n      status: Unknown\n      type: Updated\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Find the machine's hostname with the following command, and follow the ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311"},"workaround")," to see if service certificates expire on a node:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get machines.cluster.x-k8s.io -n fleet-local <machine_name> -o yaml | yq .status.nodeRef.name\n")),(0,n.kt)("p",{parentName:"li"},"Replace ",(0,n.kt)("inlineCode",{parentName:"p"},"<machine_name>")," with the machine's name from the output in the previous step."),(0,n.kt)("admonition",{parentName:"li",type:"note"},(0,n.kt)("p",{parentName:"admonition"},"If multiple nodes joined the cluster around the same time, you should perform the ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311"},"workaround")," on all those nodes.")))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/3863"},"[DOC/ENHANCEMENT] need to add cert-rotate feature, otherwise upgrade may stuck on Waiting for CAPI cluster fleet-local/local to be provisioned")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311"},"https://github.com/harvester/harvester/issues/3863#issuecomment-1539681311"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"8-the-registrysusecomharvester-betavmdplatest-image-is-not-available-in-air-gapped-environment"},"8. The ",(0,n.kt)("inlineCode",{parentName:"h3"},"registry.suse.com/harvester-beta/vmdp:latest")," image is not available in air-gapped environment"),(0,n.kt)("p",null,"Harvester does not package the ",(0,n.kt)("inlineCode",{parentName:"p"},"registry.suse.com/harvester-beta/vmdp:latest")," image in the ISO file as of v1.1.0. For Windows VMs before v1.1.0, they used this image as a container disk. However, kubelet may remove old images to free up bytes. Windows VMs can't access an air-gapped environment when this image is removed. You can fix this issue by changing the image to ",(0,n.kt)("inlineCode",{parentName:"p"},"registry.suse.com/suse/vmdp/vmdp:2.5.4.2")," and restarting the Windows VMs."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4534"},"[BUG] VMDP Image wrong after upgrade to Harvester 1.2.0"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"9-an-upgrade-is-stuck-in-the-post-draining-state"},"9. An Upgrade is stuck in the Post-draining state"),(0,n.kt)("admonition",{type:"note"},(0,n.kt)("p",{parentName:"admonition"},"This known issue is fixed in v1.2.1.")),(0,n.kt)("p",null,"The node might be stuck in the OS upgrade process if you encounter the ",(0,n.kt)("strong",{parentName:"p"},"Post-draining")," state, as shown below."),(0,n.kt)("p",null,(0,n.kt)("img",{src:a(14627).Z,width:"560",height:"801"})),(0,n.kt)("p",null,"Harvester uses ",(0,n.kt)("inlineCode",{parentName:"p"},"elemental upgrade")," to help us upgrade the OS. Check the ",(0,n.kt)("inlineCode",{parentName:"p"},"elemental upgrade")," logs to see if there are any errors."),(0,n.kt)("p",null,"You can check the ",(0,n.kt)("inlineCode",{parentName:"p"},"elemental upgrade")," logs with the following commands:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"# View the post-drain job, which should be named `hvst-upgrade-xxx-post-drain-xxx`\n$ kubectl get pod --selector=harvesterhci.io/upgradeJobType=post-drain -n harvester-system\n\n# Check the logs with the following command\n$ kubectl logs -n harvester-system pods/hvst-upgrade-xxx-post-drain-xxx\n")),(0,n.kt)("p",null,"Suppose you see the following error in the logs. An incomplete ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml")," causes this issue. "),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"Flag --directory has been deprecated, 'directory' is deprecated please use 'system' instead\nINFO[2023-09-13T12:02:42Z] Starting elemental version 0.3.1             \nINFO[2023-09-13T12:02:42Z] reading configuration form '/tmp/tmp.N6rn4F6mKM' \nERRO[2023-09-13T12:02:42Z] Invalid upgrade command setup undefined state partition \nelemental upgrade failed with return code: 33\n+ ret=33\n+ '[' 33 '!=' 0 ']'\n+ echo 'elemental upgrade failed with return code: 33'\n+ cat /host/usr/local/upgrade_tmp/elemental-upgrade-20230913120242.log\n")),(0,n.kt)("p",null,"In this case, Harvester upgrades the elemental-cli to the latest version. It will try to find the ",(0,n.kt)("inlineCode",{parentName:"p"},"state")," partition from the ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml"),". If the ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml")," is incomplete, there is a chance it will fail to find the ",(0,n.kt)("inlineCode",{parentName:"p"},"state")," partition."),(0,n.kt)("p",null,"The incomplete ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml")," will look like the following."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-yaml"},'# Autogenerated file by elemental client, do not edit\n\ndate: "2023-09-13T08:31:42Z"\nstate:\n    # we are missing `label` here.\n    active:\n        source: dir:///tmp/tmp.01deNrXNEC\n        label: COS_ACTIVE\n        fs: ext2\n    passive: null\n')),(0,n.kt)("p",null,"Remove this incomplete ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml")," file to work around this issue. (The post-draining will retry every 10 minutes)."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Remount the ",(0,n.kt)("inlineCode",{parentName:"p"},"state")," partition to RW."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"$ mount -o remount,rw /run/initramfs/cos-state\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Remove the ",(0,n.kt)("inlineCode",{parentName:"p"},"state.yaml"),"."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"$ rm -f /run/initramfs/cos-state/state.yaml\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Remount the ",(0,n.kt)("inlineCode",{parentName:"p"},"state")," partition to RO."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"$ mount -o remount,ro /run/initramfs/cos-state\n")))),(0,n.kt)("p",null,"After performing the steps above, you should pass post-draining with the next retry."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issues:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4526"},"[BUG] Upgrade stuck with first node in Post-draining state")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/rancher/elemental-toolkit/issues/1827"},"A potential bug in NewElementalPartitionsFromList which caused upgrade error code 33")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4526#issuecomment-1732853216"},"https://github.com/harvester/harvester/issues/4526#issuecomment-1732853216"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"10-an-upgrade-is-stuck-in-the-upgrading-system-service-state-due-to-the-customer-provided-ssl-certificate-without-ip-san-error-in-fleet-agent"},"10. An upgrade is stuck in the Upgrading System Service state due to the ",(0,n.kt)("inlineCode",{parentName:"h3"},"customer provided SSL certificate without IP SAN")," error in ",(0,n.kt)("inlineCode",{parentName:"h3"},"fleet-agent")),(0,n.kt)("admonition",{type:"note"},(0,n.kt)("p",{parentName:"admonition"},"This known issue is fixed in v1.2.1.")),(0,n.kt)("p",null,"If an upgrade is stuck in an ",(0,n.kt)("strong",{parentName:"p"},"Upgrading System Service")," state for an extended period, follow these steps to investigate this issue:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Find the pods related to the upgrade:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"kubectl get pods -A | grep upgrade\n")),(0,n.kt)("p",{parentName:"li"},"Example output:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"# kubectl get pods -A | grep upgrade\ncattle-system               system-upgrade-controller-5685d568ff-tkvxb                 1/1     Running     0              85m\nharvester-system            hvst-upgrade-vq4hl-apply-manifests-65vv8                   1/1     Running     0              87m  // waiting for managedchart to be ready\n..\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The pod ",(0,n.kt)("inlineCode",{parentName:"p"},"hvst-upgrade-vq4hl-apply-manifests-65vv8")," has the following loop log:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"Current version: 102.0.0+up40.1.2, Current state: WaitApplied, Current generation: 23\nSleep for 5 seconds to retry\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the status for all bundles. Note thata couple of bundles are ",(0,n.kt)("inlineCode",{parentName:"p"},"OutOfSync"),":"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"# kubectl get bundle -A\nNAMESPACE     NAME                                          BUNDLEDEPLOYMENTS-READY   STATUS\n...\nfleet-local   mcc-local-managed-system-upgrade-controller   1/1\nfleet-local   mcc-rancher-logging                           0/1                       OutOfSync(1) [Cluster fleet-local/local]\nfleet-local   mcc-rancher-logging-crd                       0/1                       OutOfSync(1) [Cluster fleet-local/local]\nfleet-local   mcc-rancher-monitoring                        0/1                       OutOfSync(1) [Cluster fleet-local/local]\nfleet-local   mcc-rancher-monitoring-crd                    0/1                       WaitApplied(1) [Cluster fleet-local/local]\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The pod ",(0,n.kt)("inlineCode",{parentName:"p"},"fleet-agent-*")," has following error log:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'fleet-agent pod log:\n\ntime="2023-09-19T12:18:10Z" level=error msg="Failed to register agent: looking up secret cattle-fleet-local-system/fleet-agent-bootstrap: Post \\"https://192.168.122.199/apis/fleet.cattle.io/    v1alpha1/namespaces/fleet-local/clusterregistrations\\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.122.199 because it doesn\'t contain any IP SANs"\n'))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the ",(0,n.kt)("inlineCode",{parentName:"p"},"ssl-certificates")," settings in Harvester:"),(0,n.kt)("p",{parentName:"li"},"From the command line:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'# kubectl get settings.harvesterhci.io ssl-certificates\nNAME               VALUE\nssl-certificates   {"publicCertificate":"-----BEGIN CERTIFICATE-----\\nMIIFNDCCAxygAwIBAgIUS7DoHthR/IR30+H/P0pv6HlfOZUwDQYJKoZIhvcNAQEL\\nBQAwFjEUMBIGA1UEAwwLZXhhbXBsZS5j...."}\n')),(0,n.kt)("p",{parentName:"li"},"From the Harvester Web UI:"),(0,n.kt)("p",{parentName:"li"},(0,n.kt)("img",{src:a(61632).Z,width:"2318",height:"512"}))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Check the ",(0,n.kt)("inlineCode",{parentName:"p"},"server-url")," setting, it is the value of VIP:"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"}," # kubectl get settings.management.cattle.io -n cattle-system server-url\nNAME         VALUE\nserver-url   https://192.168.122.199\n"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The root cause:"),(0,n.kt)("p",{parentName:"li"},"User sets the self-signed ",(0,n.kt)("inlineCode",{parentName:"p"},"ssl-certificates")," with FQDN in the Harvester settings, but the ",(0,n.kt)("inlineCode",{parentName:"p"},"server-url")," points to the VIP, the ",(0,n.kt)("inlineCode",{parentName:"p"},"fleet-agent")," pod fails to register."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},'For example: create self-signed certificate for (*).example.com\n\nopenssl req -x509 -newkey rsa:4096 -sha256 -days 3650 -nodes \\\n-keyout example.key -out example.crt -subj "/CN=example.com" \\\n-addext "subjectAltName=DNS:example.com,DNS:*.example.com"\n\nThe general outputs are: example.crt, example.key\n'))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"The workaround:"),(0,n.kt)("p",{parentName:"li"},"Update ",(0,n.kt)("inlineCode",{parentName:"p"},"server-url")," with the value of ",(0,n.kt)("inlineCode",{parentName:"p"},"https://harv31.example.com")),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre"},"# kubectl edit settings.management.cattle.io -n cattle-system server-url\nsetting.management.cattle.io/server-url edited\n...\n\n# kubectl get settings.management.cattle.io -n cattle-system server-url\nNAME         VALUE\nserver-url   https://harv31.example.com\n")),(0,n.kt)("p",{parentName:"li"},"After the workaround is applied, the ",(0,n.kt)("inlineCode",{parentName:"p"},"fleet-agent")," pod is replaced by Rancher automatically and registers successfully, the upgrade continues."))),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Related issue:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4519"},"[BUG] Upgrade to Harvester 1.2.0 fails in fleet-agent due to customer provided SSL certificate without IP SAN")))),(0,n.kt)("li",{parentName:"ul"},"Workaround:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/harvester/harvester/issues/4519#issuecomment-1727132383"},"https://github.com/harvester/harvester/issues/4519#issuecomment-1727132383"))))),(0,n.kt)("hr",null),(0,n.kt)("h3",{id:"11-an-upgrade-is-denied-due-to-managed-chart-rancher-monitoring-crd-is-not-ready"},"11. An upgrade is denied due to ",(0,n.kt)("inlineCode",{parentName:"h3"},"managed chart rancher-monitoring-crd is not ready")),(0,n.kt)("p",null,"When you ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/upgrade/index#start-an-upgrade"},"start an upgrade")," and Harvester returns such an error message: ",(0,n.kt)("inlineCode",{parentName:"p"},'admission webhook "validator.harvesterhci.io" denied the request: managed chart rancher-monitoring-crd is not ready, please wait for it to be ready'),". Please follow this ",(0,n.kt)("a",{parentName:"p",href:"/v1.2/troubleshooting/monitoring#the-state-of-the-rancher-monitoring-crd-managedchart-is-modified"},"troubleshooting"),"."),(0,n.kt)("hr",null))}m.isMDXComponent=!0},4947:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/2894-deadline-8fbfd53960ef87f904f6a893a4a0bfcd.png"},30150:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/3730-stuck-c48392f50cb65bcfe1e0e823d6696e3e.png"},25067:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/3839-error-6dc815a5f5edbb60c92750f1a65c48d6.png"},84940:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4246-pending-replica-27cff1f4b47113bcb0a5fc207ad54e61.png"},65894:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4246-pending-7c411c93a62433d215b54e21f938f3ef.png"},83918:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4246-upgrade-repo-pod-9a796ae8bfcd218dc2484f47841d8b44.png"},35710:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4249-adjust-volume-replica-4a57d4c639f7a90f40e6b22e40b7fdf3.png"},42702:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4484-apply-manifests-stuck-2731b903408d51749964176fee50985a.png"},61632:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/4519-harvester-settings-ssl-certificates-bce63ec0bd7f1781bd2c064606ea3b10.png"},14627:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/stuck-in-post-draining-1005eb89a8619dd617925a3cf805f8e9.png"}}]);